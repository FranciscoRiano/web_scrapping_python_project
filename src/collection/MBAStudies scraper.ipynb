{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prGJWr41awM9"
   },
   "source": [
    "# <span style=\"color:blue\">Scraping information on MBA programmes from MBAstudies.com</span>\n",
    "## About MBAstudies.com\n",
    "MBAstudies.com allows students to browse thousands of graduate degrees from around the world. The website provides information on several different types of MBAs such as Master of Business Administration (MBA), Executive MBAs (EMBAs), Executive Courses, Online Degrees, etc.\n",
    "\n",
    "## Scope of the project\n",
    "For the Online Data Collection & Management course at Tilburg University, our team developed a scraper, which collects general information on the Master of Business Administration (MBA) programmes (to be found at www.mbastudies.com/MBA/) per each specified country. The list of countries can be modified according to the researcher's interest. In our project, we are going to include the Group of Seven (G7) countries: Canada, France, Germany, Italy, Japan, the United Kingdom, and the United States.\n",
    "\n",
    "The following information is going to be retrieved from each programme: All Locations, Duration, Earliest Start Date, Application Deadline, Languages, Study Type, Pace, and Tuition Fees.\n",
    "\n",
    "## Workflow\n",
    "The following workflow was applied in this project:\n",
    "1. Installing and importing required packages.\n",
    "2. Seed generation: \n",
    "    - generating base URLs for each country\n",
    "    - generating all pages URLs for each country\n",
    "    - scraping links to programmes from each page\n",
    "3. Scraping programme information from each programme URL.\n",
    "4. Saving data: \n",
    "    - raw data in a json format\n",
    "    - dataframe in a csv file for an analysis in R\n",
    "\n",
    "Furthermore, the data from the csv file is going to be cleaned and preapred for the statistical analysis in R.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aVDe_TZjawNA"
   },
   "source": [
    "## 1. Importing packages\n",
    "The following packages are needed to run this scraper, so make sure you install and/or load them first. The follwing commands in the Command Prompt (on Windows) will allow you to install the essencial libraries/tools:\n",
    "- pip install bs4\n",
    "- pip install webdriver-manager\n",
    "- pip install selenium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CWcLTIFEawNB"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # for getting data out of HTML, XML, and other markup languages\n",
    "import requests # for sending HTTP/1.1 requests\n",
    "from requests import models\n",
    "import re # to check if a string contains the specified search pattern\n",
    "import pandas as pd # data analysis library\n",
    "from time import sleep # to pause the execution of future commands for a given number of seconds\n",
    "import json # to convert the python dictionary into a JSON string that can be written into a file\n",
    "import csv # for creating a csv file\n",
    "import itertools # provides various functions that work on iterators to produce complex iterators\n",
    "import selenium # for controlling web browsers through programs\n",
    "from selenium.webdriver.common.by import By # for finding elements on the website by xpath, class name, etc.\n",
    "from selenium import webdriver # for performing browser automation\n",
    "from webdriver_manager.chrome import ChromeDriverManager # for getting the webdriver\n",
    "from selenium.webdriver.chrome.service import Service #used for the webdriver to work properly\n",
    "import os # for identifying the current directory (used when downloading files to specific directories)\n",
    "from datetime import datetime # for creating a timestamp of current time and date when downloading csv/json file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6YlLsLgawNC"
   },
   "source": [
    "## 2. Seed generation\n",
    "The seed generation step consists of several substeps. The ultimate goal is to get a list of URLs of all MBA programmes in selected countries.\n",
    "Firstly, we define the 'base url' which is the part of the url that is the same for different countries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "VvC6Jt3wawNC"
   },
   "outputs": [],
   "source": [
    "base_url = 'https://www.mbastudies.com/MBA/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7cDDolHRawND"
   },
   "source": [
    "Secondly, we define a list of countries that we want to scrape data from. It is important that they are written in exactly the same way as they are presented in the url. The list of countries can be adjusted manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Q5bXTlN8awND"
   },
   "outputs": [],
   "source": [
    "countries_list = ['Canada', 'France', 'Germany', 'Italy', 'Japan', 'United-Kingdom', 'USA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8ncsi1SawNE"
   },
   "source": [
    "Thirdly, a function is defined where the input is the base url and the countries list. The function consists of a for loop which connects the base URL with each country name from the list of countries. The result of the function is a list with URLs for all countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IjF_4ZaBawNE"
   },
   "outputs": [],
   "source": [
    "def generate_country_urls(base_url, country):\n",
    "    '''\n",
    "    Function generating links for each country from the list.\n",
    "    '''\n",
    "    page_urls = []\n",
    "    for country in countries_list:\n",
    "        country_url = base_url + country\n",
    "        page_urls.append(country_url)\n",
    "    return page_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gb4CiSm_awNF"
   },
   "source": [
    "A list of URLs for all countries is stored in \"country_urls\" variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bh4aTqMFawNF",
    "outputId": "28e7936a-1748-4218-f2df-e6270addca4d"
   },
   "outputs": [],
   "source": [
    "country_urls = generate_country_urls(base_url, countries_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A85IyJjzawNG"
   },
   "source": [
    "Next, we need to retrieve the total number of pages in the country URLs in order to be able to scrape all programme URLs from each page.\n",
    "\n",
    "Therefore, a function that defines the total number of pages in the country URLs is created. The total number of pages is displayed on each country page and stored as an attribute of class \"pagination mx-auto\". The common text for all pages is \"Page 1 of [max number of pages]\". Therefore, the total number of pages is given. In the case when there's only one page in a country URL, this text is not displayed. Therefore, if the attribute is not found on the website, it can be assumed that there's only one page in that country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RWqsz7AbawNG",
    "outputId": "d1ed7c99-daef-4805-f116-cdd8f3e7f9c4"
   },
   "outputs": [],
   "source": [
    "def total_pages(country_url):\n",
    "    '''\n",
    "    Function defining the number of pages per country.\n",
    "    country_url: base URL for a country \n",
    "    '''\n",
    "    result = requests.get(country_url)\n",
    "    sleep(2)\n",
    "    src = result.text\n",
    "    soup = BeautifulSoup(src, \"html.parser\")\n",
    "    \n",
    "    # searching for the \"page number\" attribute on the website:\n",
    "    check_for_pages = soup.find(attrs={\"class\":\"pagination mx-auto\"}).find(\"span\") \n",
    "    \n",
    "    if check_for_pages:\n",
    "        text = check_for_pages.next # getting the text from that attribute, if found on the website\n",
    "    else:\n",
    "        text = \"Page 1 of 1\" # if the attribute is not found, there's only one page in that country URL\n",
    "    \n",
    "    clean = text.replace(\"Page 1 of \",\"\") # removing unnecessary text\n",
    "    total_pages = int(clean) # transforming variable to an integer\n",
    "    return total_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Wwgf3UnawNH"
   },
   "source": [
    "Furthermore, a function is defined where the input is the country URL for which we want to generate pages URLs and the total number of pages in that country. The function consists of a for loop which connects the country URL with the page number until the last page is reached. As a result, we get a list of all pages per country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8qTh5ieAawNH",
    "outputId": "bd05725c-2774-4cf5-c75a-9f9430f9cb33"
   },
   "outputs": [],
   "source": [
    "def generate_page_urls(country_url, num_pages):\n",
    "    '''\n",
    "    Function generating URLs for all pages in the country URL.\n",
    "    country_url: base URL for a country\n",
    "    num_pages: the total number of pages in that country URL\n",
    "    '''\n",
    "    all_country_urls = []  \n",
    "    for counter in range(1, num_pages+1):\n",
    "        full_url = country_url + \"/?page=\" + str(counter)\n",
    "        all_country_urls.append(full_url)\n",
    "    return all_country_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The page URLs for each coutry are generated with the for loop and stored in a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lXbBrt_fKNWE",
    "outputId": "316fcaf8-93d9-461a-c388-843647b12de1"
   },
   "outputs": [],
   "source": [
    "one_list = []\n",
    "num_countries = len(countries_list)\n",
    "\n",
    "for i in range(num_countries): \n",
    "    all_links = generate_page_urls(country_urls[i], total_pages(country_urls[i]))\n",
    "    one_list.append(all_links)\n",
    "    one_list2 = list(itertools.chain.from_iterable(one_list)) # to remove a list from each URL and store them in a signle list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, from all pages in each country, the URLs to study programmes are scraped. These links will be used to scrape the information about these MBA programmes. The result of the function is stored in a list \"programs\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ziWh52nPawNJ",
    "outputId": "0a19f5e1-c477-41e6-f3f8-c68de435435b"
   },
   "outputs": [],
   "source": [
    "def scrape_final_urls(list_links):\n",
    "    '''\n",
    "    Function scraping the URLs to MBA programmes.\n",
    "    list_links: URL of page for which the programme URLs should be scraped\n",
    "    '''\n",
    "    programs = []\n",
    "    for url in list_links: \n",
    "        res = requests.get(url)\n",
    "        sleep(2)\n",
    "        soup2 = BeautifulSoup(res.text, \"html.parser\")\n",
    "             \n",
    "        lenght = len(soup2.find_all(class_=\"program_title\"))\n",
    "    \n",
    "        for x in range(lenght):\n",
    "            prog = soup2.find_all(class_=\"program_title\")[x].find(\"a\").attrs[\"href\"]\n",
    "            programs.append(prog)\n",
    "  \n",
    "    return programs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of all programme URLs is stored in the \"program_urls\" variable. Some of the listed programmes do not have a link, and therefore, empty values are removed from the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "program_urls = scrape_final_urls(one_list2)\n",
    "while(\"\" in program_urls) :\n",
    "    program_urls.remove(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCu3QvLREln0"
   },
   "source": [
    "## 3. Scraping programme information\n",
    "After we have collected all programme URLs per each country and per each page, we can proceed with retrieveing programme information from the website. A function which scrapes general information from programme URLs is created. The elements on the website are found by xpath."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 99.0.4844\n",
      "Get LATEST chromedriver version for 99.0.4844 google-chrome\n",
      "Driver [C:\\Users\\ambro\\.wdm\\drivers\\chromedriver\\win32\\99.0.4844.51\\chromedriver.exe] found in cache\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "def get_and_parse(urls, sys_null=None):\n",
    "    \"\"\"\n",
    "    Function scraping general information on MBA programmes.\n",
    "    urls: programme URL that should be scraped\n",
    "    \"\"\"\n",
    "    program_information = []\n",
    "    for url in urls:\n",
    "        driver.get(url)\n",
    "        sleep(2)\n",
    "        field_names = driver.find_elements_by_xpath(\"//div/strong\")\n",
    "        fields = driver.find_elements_by_xpath(\"//div[@class='cell auto']\")\n",
    "        fields = [n.text for n in fields]\n",
    "        fields2 = {\n",
    "\n",
    "        }\n",
    "\n",
    "        for n in fields:\n",
    "            try:\n",
    "                fields2[n.split(\"\\n\")[0]] = n.split(\"\\n\")[1]\n",
    "            except:\n",
    "                pass\n",
    "            fields = fields2\n",
    "        program_information.append(fields)\n",
    "        fields[\"url\"]=url\n",
    "        \n",
    "    return program_information\n",
    "\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general information about all MBA programmes is stored as a variable \"final_data\". Furthermore, this variable is transformed into the dataframe and stored as \"final_dataframe\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ambro\\AppData\\Local\\Temp/ipykernel_19860/3423834667.py:12: DeprecationWarning: find_elements_by_xpath is deprecated. Please use find_elements(by=By.XPATH, value=xpath) instead\n",
      "  field_names = driver.find_elements_by_xpath(\"//div/strong\")\n",
      "C:\\Users\\ambro\\AppData\\Local\\Temp/ipykernel_19860/3423834667.py:13: DeprecationWarning: find_elements_by_xpath is deprecated. Please use find_elements(by=By.XPATH, value=xpath) instead\n",
      "  fields = driver.find_elements_by_xpath(\"//div[@class='cell auto']\")\n"
     ]
    }
   ],
   "source": [
    "final_data = get_and_parse(program_urls) # saving the output as variable 'final_data'\n",
    "final_dataframe = pd.DataFrame(final_data) # dataframe with all data for programmes in selected countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Saving data\n",
    "After the data is gathered, it will be downloaded as a json and csv file. The name of the file will consist of MBAStudies + current time and date. If you want to specify the directory where the files should be saved, please, uncomment the code and provide the correct path on your device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir(\"C:/Users/ambro/Desktop\") # specify, where you want the output to be saved\n",
    "filename = datetime.now().strftime('MBAStudies_%H%M_%m%d%Y.csv')\n",
    "final_dataframe.to_csv(filename, encoding='utf-8')\n",
    "\n",
    "filename2 = datetime.now().strftime('MBAStudies_%H%M_%m%d%Y.json')\n",
    "with open(filename2.format(1), 'w', encoding='utf-8') as file:\n",
    "    json.dump(final_data, file, ensure_ascii=False, indent=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping finished and data saved\n"
     ]
    }
   ],
   "source": [
    "print(\"Scraping finished and data saved\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copia de Scraper 1.1.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "a4dfe1d041267b005702b509a1a133bd704b37c8839fdb81a9d0bad63e8f8ac7"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
